{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "\n",
        "<div class=\"markdown-google-sans\">\n",
        "<h2>Pinecone-PDF</h2>\n",
        "</div>\n",
        "\n",
        "**Building an Interactive Chatbot with Langchain, ChatGPT, Pinecone, and Streamlit**\n",
        "\n",
        "creating a highly efficient chatbot that can answer queries from its own documents or knowledge base. This chatbot, leveraging the power of advanced language models, can also respond to follow-up questions from the users, ensuring a seamless and interactive user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Install Dependencies**\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "OrN9jexXL_OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Train Dataset**\n",
        "</div>\n",
        "\n",
        "1.   Data Collection\n",
        "\n",
        "> The project began with collecting a PDF file containing frequently asked questions and their corresponding answers. This PDF file served as the knowledge base for the chatbot.\n",
        "\n",
        "> store PDF files in `data` folder\n"
      ],
      "metadata": {
        "id": "2fGIT0HrMIRj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Training the Model**\n",
        "</div>\n",
        "\n",
        "\n",
        "1.   **Loading documents from a directory with LangChain**\n",
        "> The first step in the `app.py` script involves loading the documents from a directory. We use the `DirectoryLoader` class provided by LangChain to achieve this. This class accepts a directory as input and loads all the documents present in it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "directory = './data'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "s78n2okOPtZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdRyKR44dcNI"
      },
      "source": [
        "\n",
        "\n",
        "2.   **Splitting documents**\n",
        "> After loading the documents, the script proceeds to split these documents into smaller chunks. The size of the chunks and the overlap between these chunks can be defined by the user. This is done to ensure that the size of the documents is manageable and that no relevant information is missed out due to the splitting. The RecursiveCharacterTextSplitter class from LangChain is used for this purpose\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4HZx7Gndbrh"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_docs(documents,chunk_size=500,chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_kCnsPUqS6o"
      },
      "source": [
        "\n",
        "3.   **Creating embeddings**\n",
        "\n",
        ">Once the documents are split, we need to convert these chunks of text into a format that our AI model can understand. This is done by creating embeddings of the text using SentenceTransformerEmbeddings class provided by LangChain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "LdmIMBOnOGV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "4.   **Storing embeddings in Pinecone**\n",
        "\n",
        ">After the embeddings are created, they need to be stored in a place from where they can be easily accessed and searched. Pinecone is a vector database service that is perfect for this task. The embeddings are stored in Pinecone using the Pinecone class from LangChain.\n"
      ],
      "metadata": {
        "id": "SZTFhJc6OKxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "pinecone.init(\n",
        "    api_key=\"\",  # find at app.pinecone.io\n",
        "    environment=\"gcp-starter\"  # next to api key in console\n",
        ")\n",
        "index_name = \"faq\"\n",
        "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "rvSMVwB-ORoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This completes the process of document indexing, and we are now ready to move to the main application of our chatbot."
      ],
      "metadata": {
        "id": "1_IuGor6O21L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Building the Chatbot Application with Streamlit**\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "1.   With the indexed documents in place, the main part of our task is to build the chatbot application itself. We use Streamlit to create a seamless interactive interface for the chatbot.We accomplish this using the `main.py` file.\n",
        "\n"
      ],
      "metadata": {
        "id": "g8bsrJILRqSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder\n",
        ")\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "from utils import *\n",
        "\n",
        "st.subheader(\"Chatbot with Langchain, ChatGPT, Pinecone, and Streamlit\")\n",
        "\n",
        "if 'responses' not in st.session_state:\n",
        "    st.session_state['responses'] = [\"How can I assist you?\"]\n",
        "\n",
        "if 'requests' not in st.session_state:\n",
        "    st.session_state['requests'] = []\n",
        "\n",
        "if 'buffer_memory' not in st.session_state:\n",
        "            st.session_state.buffer_memory=ConversationBufferWindowMemory(k=3,return_messages=True)\n",
        "\n",
        "\n",
        "system_msg_template = SystemMessagePromptTemplate.from_template(template=\"\"\"Answer the question as truthfully as possible using the provided context,\n",
        "and if the answer is not contained within the text below, say 'I don't know'\"\"\")\n",
        "\n",
        "\n",
        "human_msg_template = HumanMessagePromptTemplate.from_template(template=\"{input}\")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([system_msg_template, MessagesPlaceholder(variable_name=\"history\"), human_msg_template])\n"
      ],
      "metadata": {
        "id": "BP9shTAVPVog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Session State Initialisation**: Firstly, we initialise two lists 'responses' and 'requests' within Streamlit's session state. These lists store the history of bot responses and user requests respectively.\n",
        "*   **ConversationBufferWindowMemory**: This memory structure is instantiated with a size of 3, meaning that our chatbot would remember the last three interactions, keeping a manageable memory size for efficiency.\n",
        "*   **PromptTemplate Construction**: We construct a PromptTemplate for our chatbot. The template contains instructions to the language model (LLM), providing structure and context to the input for the LLM to generate a response. Langchain provides different types of MessagePromptTemplate, which includes AIMessagePromptTemplate, SystemMessagePromptTemplate, and HumanMessagePromptTemplate, for creating different types of messages."
      ],
      "metadata": {
        "id": "clxwVakwQDsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2.   Creating the User Interface\n",
        ">The Streamlit library allows us to quickly build a user-friendly interface for our chatbot application. The `st.title` function is used to display the chatbot's title at the top of the interface. The user's queries and the chatbot's responses are displayed in a conversation format using the `st.container` and `st.text_input` functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zq2LbLymQWXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st.title(\"Langchain Chatbot\")\n",
        "...\n",
        "response_container = st.container()\n",
        "textcontainer = st.container()\n",
        "...\n",
        "with textcontainer:\n",
        "    query = st.text_input(\"Query: \", key=\"input\")\n",
        "    ...\n",
        "with response_container:\n",
        "    if st.session_state['responses']:\n",
        "        for i in range(len(st.session_state['responses'])):\n",
        "            message(st.session_state['responses'][i],key=str(i))\n",
        "            if i < len(st.session_state['requests']):\n",
        "                message(st.session_state[\"requests\"][i], is_user=True,key=str(i)+ '_user')\n"
      ],
      "metadata": {
        "id": "zyCqPKDYQnMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.   **Initializing the Language Model and Conversation**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r21p4FZLQv9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=\"\")\n",
        "...\n",
        "conversation = ConversationChain(memory=st.session_state.buffer_memory, prompt=prompt_template, llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "x-lugndYQ8sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4.   **Generating Responses**\n",
        ">When the user inputs a query, the chatbot uses the `predict` method to generate a response. The response is then displayed in the chat interface."
      ],
      "metadata": {
        "id": "OpvrIxDARCou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if query:\n",
        "    with st.spinner(\"typing...\"):\n",
        "        ...\n",
        "        response = conversation.predict(input=f\"Context:\\n {context} \\n\\n Query:\\n{query}\")\n",
        "    st.session_state.requests.append(query)\n",
        "    st.session_state.responses.append(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "vV_RguCqRHK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Refining Queries and Finding Matches with Utility Functions**\n",
        "</div>\n",
        "\n",
        "Once our chatbot is operational, we need to ensure that it can effectively process user queries and find relevant responses. This is achieved through a set of utility functions defined in `utils.py`. Here, we describe the purpose of these functions and their roles in the application.\n",
        "\n",
        "\n",
        "1.   **Refining Queries with OpenAI**\n",
        ">The `query_refiner` function is used to take the user's query and refine it to ensure it's optimal for providing a relevant answer. It uses OpenAI's DaVinci model to refine the query based on the current conversation log.\n"
      ],
      "metadata": {
        "id": "ytjV-zotRctY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_refiner(conversation, query):\n",
        "    response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=f\"Given the following user query and conversation log, formulate a question that would be the most relevant to provide the user with an answer from a knowledge base.\\n\\nCONVERSATION LOG: \\n{conversation}\\n\\nQuery: {query}\\n\\nRefined Query:\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        "    )\n",
        "    return response['choices'][0]['text']"
      ],
      "metadata": {
        "id": "wqdEVlYWR6In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2.   **Finding Matches in Pinecone Index**\n",
        ">The find_match function is used to find the most relevant documents that match the user's query. It uses the Pinecone vector index to find matches and returns the most relevant text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RWEAGq2GR7Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_match(input):\n",
        "    input_em = model.encode(input).tolist()\n",
        "    result = index.query(input_em, top_k=2, includeMetadata=True)\n",
        "    return result['matches'][0]['metadata']['text']+\"\\n\"+result['matches'][1]['metadata']['text']"
      ],
      "metadata": {
        "id": "eoYuqAhjSCqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.   **Tracking the Conversation**\n",
        ">The get_conversation_string function is used to keep track of the ongoing conversation. It generates a string of the conversation log, including both the user's queries and the chatbot's responses."
      ],
      "metadata": {
        "id": "sPQXdmlESJNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conversation_string():\n",
        "    conversation_string = \"\"\n",
        "    for i in range(len(st.session_state['responses'])-1):\n",
        "        conversation_string += \"Human: \"+st.session_state['requests'][i] + \"\\n\"\n",
        "        conversation_string += \"Bot: \"+ st.session_state['responses'][i+1] + \"\\n\"\n",
        "    return conversation_string"
      ],
      "metadata": {
        "id": "O4eYntMsSUWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">With these utility functions, the chatbot can not only generate responses but also refine the user's queries and find the most relevant answers. This ensures a more effective and user-friendly chatbot experience."
      ],
      "metadata": {
        "id": "BThE50epSVRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Example**\n",
        "</div>\n",
        "\n",
        "1.   Which Languages supported?\n",
        ">The supported languages are Hindi and English. Tamil, Telugu, Malayalam, Kannada, Gujarati, Marathi, and Bengali will be available soon.\n",
        "2.   Does it supported in Desktop Offfline mode?\n",
        ">Yes, it is supported in Desktop Offline mode for certain use cases like ATM Machines.\n",
        "\n",
        "\n",
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Implementation Demo link**\n",
        "</div>\n",
        "  \n",
        "1.   link: http://122.169.118.18:8501/\n",
        "\n"
      ],
      "metadata": {
        "id": "Uk7Xi5_kSuSz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
